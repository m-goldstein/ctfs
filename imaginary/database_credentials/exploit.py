import requests
import json
import time
import base64 as b64
import re
host = 'https://api.github.com/repos/Et3rnos/ImaginaryCTF'
token = ''
pat = {'Authorization':f'token {token}'}
#pat = {}
parse = lambda x,y: [e[y] for e in x]
events_list = ['PushEvent','MergeEvent','IssueCommentEvent','IssuesEvent','DeleteEvent'] 
regex_str = r'user=ictf\w+'
regex = re.compile(regex_str)
matches = []
def check_for_match(s='',regex=regex):
    global matches
    matches += [s[e.start():e.start()+100] for e in regex.finditer(s)]
    return

def get(host):
    global regex,matches
    try:
        r = requests.get(host,headers=pat)
        data = r.json()
        if 'rate limit exceeded' in r.text and 'message' in data:
            print(f'[get] Error: {data["message"][:44]}')
            return {}
        check_for_match(s=r.text,regex=regex)
        return data
    except Exception as e:
        print(f'[get] Exception: {e}')

def preprocess(host=host,endpoint=''):
    global events_list
    try:
        if (host+endpoint).endswith('/events'):
            tmp = get(host+endpoint)
            data = [e for e in tmp if e['type'] in events_list and 'ImaginaryCTF' in e['repo']['name'] ] if tmp != {} else []
            return data
        else:
            return get(host+endpoint)
    except Exception as _e:
        print(f'[preprocess] Exception: {_e}')

def gen_commit_hrefs(host=host,events=[],commit_hrefs=[]):
    events += preprocess(host,endpoint='/events')
    for e in events:
        if 'before' in e['payload']:
            parent = e['payload']['before']
            if f'{host}/commits/{parent}' not in commit_hrefs:
                commit_hrefs += [f'{host}/commits/{parent}']
        if 'commits' in e['payload']:
            commits = e['payload']['commits']
            for c in commits:
                if c['url'] not in commit_hrefs:
                    commit_hrefs += [c['url']]
        if e['type'] in ['IssuesEvent','IssueCommentEvent']:
            if 'issue' in e['payload']:
                if 'comments_url' in e['payload']['issue']:
                    get(e['payload']['issue']['comments_url'])
                if 'events_url' in e['payload']['issue']:
                    get(e['payload']['issue']['events_url'])
                if 'title' in e['payload']['issue']:
                    check_for_match(s=e['payload']['issue']['title'])
                if 'body' in e['payload']['issue']:
                    check_for_match(s=e['payload']['issue']['body'])
    return list(events),list(commit_hrefs)

def process_commit(commit='',trees=[],patches=[]):
    stack = []
    parents = []
    try:
        data = get(commit)
        if 'comments_url' in data:
            comments = get(data['comments_url'])
        if 'commit' in data and 'message' in data['commit']:
            check_for_match(s=data['commit']['message'])
        if 'parents' in data:
            parents = [e['url'] for e in data['parents']]
        tree = data['commit']['tree']['url']
        if tree not in trees:
            trees += [tree]
        for e in data['files']:
            if 'contents_url' in e:
                contents = get(e['contents_url'])
                if '_links' in contents:
                    links = contents['_links']
                    if 'git' in links:
                        blob = get(links['git'])
                        try:
                            _ = e
                            payload = b64.b64decode(blob['content']).decode() if 'content' in blob else ''
                        except Exception as _e:
                            print(f'[process_commit][b64decode] Exception: {_e}')
                            payload = blob['content'] if 'content' in blob else ''
                            print(f'[process_commit][b64decode] using blob: {payload}')
                        finally:
                            check_for_match(s=payload)
                            tmp = (_['sha'],payload)
                            if tmp[0] not in parse(patches, 0):
                                patches += [tmp]
            #if 'patch' in e.keys():
            #    patch = (e['sha'],e['patch'])
            #elif 'contents_url' in e.keys():
            #    patch = (e['sha'],e['contents_url'])
            # 
            # if patch[0] not in parse(patches,0):
            #     patches += [patch]
    except Exception as _e:
        print(f'[process_commit] Exception: {_e}')
    return list(trees),list(patches),list(parents)

def gen_trees(commits=[],trees=[],patches=[]):
    seen = []
    _commits = list(commits)
    while _commits:
        commit = _commits.pop()
        t,p,prevs = process_commit(commit=commit)
        seen += [commit]
        for e in prevs:
            if e not in seen and e not in _commits:
                _commits += [e] # search space frontier for the DFS
                commits += [e]  # entire space for global context
        for e in t:
            if e not in trees:
                trees += [e]
        for e in p:
            if e[0] not in parse(patches,0):
                patches += [e]
        print(f'processed commit url: commits/{commit.rsplit("/")[-1]}')
    return list(trees),list(patches)

def process_tree(tree='',blobs=[]):
    stack = []
    try:
        data = get(tree)
        t = [e for e in data['tree']]
        for e in t:
            sha = e['sha']
            fname = e['path']
            url = e['url']
            blob = (sha,fname,url)
            if e['type'] == 'blob':
                if blob[0] not in parse(blobs,0):
                    blobs += [blob]
            elif e['type'] == 'tree' and e['url'] not in stack:
                stack.append(e['url'])
    except Exception as _e:
        print(f'[process_tree] Exception: {_e}')
        print(f'[process_tree] {data=}')
    return list(blobs),list(stack)

def process_metadata(trees=[], queries=[]):
    seen = []
    for tree in trees:
        try:
            blob,refs = process_tree(tree=tree)
            for e in refs:
                if e not in trees:
                    trees += [e]
            urls = parse(blob,2)
            for e in urls:
                if e in seen:
                    continue
                data = get(e)
                sha = data['sha']
                payload = data['content']
                decoded = b64.b64decode(payload)
                entry = (sha,decoded)
                print(f'entry: {sha=}')
                if entry[1] not in parse(blobs,1):
                    blobs += [entry]
                print(f'processed blob url: blobs/{e.rsplit("/")[-1]}')
                seen.append(e)
        except Exception as _e:
            print(f'[process_blobs] Exception: {_e}')
        print(f'processed tree url: trees/{tree.rsplit("/")[-1]}')
    return list(blobs)
"""
def process_blobs(trees=[], blobs=[]):
    seen = []
    for tree in trees:
        try:
            blob,refs = process_tree(tree=tree)
            for e in refs:
                if e not in trees:
                    trees += [e]
            urls = parse(blob,2)
            for e in urls:
                if e in seen:
                    continue
                data = get(e)
                sha = data['sha']
                payload = data['content']
                decoded = b64.b64decode(payload)
                entry = (sha,decoded)
                print(f'entry: {sha=}')
                if entry[1] not in parse(blobs,1):
                    blobs += [entry]
                print(f'processed blob url: blobs/{e.rsplit("/")[-1]}')
                seen.append(e)
        except Exception as e:
            print(f'[process_blobs] Exception: {e}')
        print(f'processed tree url: trees/{tree.rsplit("/")[-1]}')
    return list(blobs)
"""

events,commits = gen_commit_hrefs()

print(f'generated initial list of {len(commits)} commits')
trees,patches = gen_trees(commits=commits)
print(f'expanded commits list to {len(commits)} entries')
#patch_bodies = parse(patches,1)
print(f'generated list of {len(trees)} trees')
#blobs = process_blobs(trees=trees)
#print(f'generated list of {len(blobs)} blobs')
